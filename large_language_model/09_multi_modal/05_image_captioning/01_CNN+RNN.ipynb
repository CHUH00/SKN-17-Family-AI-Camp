{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AGhRzfa3wU5"
      },
      "source": [
        "# Image Captioning 기본 구조: CNN + RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z1QwNfDf4fAB"
      },
      "outputs": [],
      "source": [
        "# 이미지 전처리\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "image = Image.open(\"test.jpg\").convert(\"RGB\")\n",
        "image_tensor = transform(image).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2AidHOui3ld3"
      },
      "outputs": [],
      "source": [
        "# 단어사전\n",
        "vocab = {\n",
        "    0: \"<pad>\",\n",
        "    1: \"<start>\",\n",
        "    2: \"<end>\",\n",
        "    3: \"a\",\n",
        "    4: \"dog\",\n",
        "    5: \"is\",\n",
        "    6: \"sitting\",\n",
        "    7: \"on\",\n",
        "    8: \"grass\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "T7DggysX9zqF",
        "outputId": "a05ba90c-0e9b-4f7e-dfa7-fafac946b4e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 528M/528M [00:08<00:00, 67.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# VGG 모델 로드 (이미지로부터 특징 추출)\n",
        "from torchvision.models import vgg16\n",
        "import torch\n",
        "\n",
        "vgg = vgg16(pretrained=True).features\n",
        "for param in vgg.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "with torch.no_grad():\n",
        "  features = vgg(image_tensor)\n",
        "  features = features.view(features.size(0), -1).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt_1s6YB_I2n"
      },
      "outputs": [],
      "source": [
        "# 단어사전을 토대로 학습 입/출력 데이터 생성\n",
        "caption = [1, 3, 4, 5, 6, 7, 8, 2]\n",
        "input_seq = torch.tensor([caption[:-1]])\n",
        "target_seq = torch.tensor([caption[1:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sv1fo-g_5tN"
      },
      "outputs": [],
      "source": [
        "# RNN 계열 Caption 생성 모델 생성 (이미지 특징 + 이전 단어 -> 다음 단어 예측)\n",
        "import torch.nn as nn\n",
        "\n",
        "class CaptionGenerator(nn.Module):\n",
        "  def __init__(self, feature_dim, embed_dim, hidden_dim, vocab_size):\n",
        "    super(CaptionGenerator, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "    self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
        "    self.init_linear = nn.Linear(feature_dim, embed_dim)\n",
        "\n",
        "  def forward(self, features, captions):\n",
        "    embedded_features = self.init_linear(features)\n",
        "    embeds = self.embed(captions)\n",
        "    inputs = torch.cat((embedded_features, embeds), dim=1)\n",
        "    hiddens, _ = self.lstm(inputs)\n",
        "    outputs = self.decoder(hiddens)\n",
        "    return outputs[:, 1:, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR6ZXVTnCEy2"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# 모델 학습\n",
        "model = CaptionGenerator(feature_dim=25088, embed_dim=256, hidden_dim=512, vocab_size=len(vocab))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(20):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  outputs = model(features, input_seq)\n",
        "  loss = criterion(outputs.squeeze(0), target_seq.squeeze(0))\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "occFbUepD7Gs",
        "outputId": "558020c1-caf9-4464-a532-b8be1d5902dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "생성된 캡션: a dog is sitting on grass\n"
          ]
        }
      ],
      "source": [
        "# 모델 예측\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  generated = []\n",
        "  input_word = torch.tensor([[1]])\n",
        "  embed_feat = model.init_linear(features)\n",
        "  hidden = None\n",
        "\n",
        "  for _ in range(10):\n",
        "    embed_input = model.embed(input_word)\n",
        "    lstm_input = torch.cat((embed_feat, embed_input), dim=1) if len(generated) == 0 else embed_input\n",
        "    out, hidden = model.lstm(lstm_input, hidden)\n",
        "    pred = model.decoder(out[:, -1, :])\n",
        "    pred_id = pred.argmax(dim=-1).item()\n",
        "\n",
        "    if pred_id == 2:\n",
        "      break\n",
        "\n",
        "    generated.append(pred_id)\n",
        "    input_word = torch.tensor([[pred_id]])\n",
        "    embed_feat = None\n",
        "\n",
        "  sentence = \" \".join([vocab[idx] for idx in generated])\n",
        "  print(\"생성된 캡션:\", sentence)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
