{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47c6510",
   "metadata": {},
   "source": [
    "# 사용자 선호에 맞는 시 창작 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd248291",
   "metadata": {},
   "source": [
    "### 0. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea31cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install typing_extensions pydantic openai\n",
    "!pip install datasets transformers peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769dd55b",
   "metadata": {},
   "source": [
    "### 1. 지도학습 (기반모델 Q-LoRA 파인튜닝)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c599c85",
   "metadata": {},
   "source": [
    "##### (1) 학습용 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e662fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset_path = \"./korean_poetry_dataset.json\"\n",
    "\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    poem_data = json.load(f)\n",
    "\n",
    "processed_data = [\n",
    "    {\"topic\": item[\"text\"][\"topic\"], \"poem\": item[\"text\"][\"poem\"]}\n",
    "    for item in poem_data\n",
    "]\n",
    "\n",
    "train_dataset = Dataset.from_list(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da133e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 로드\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수: 토큰화 + 라벨링\n",
    "def preprocess_text(sample):\n",
    "    input_texts = [f\"주제: {t}\\n\\n시: {p}\" for t, p in zip(sample[\"topic\"], sample[\"poem\"])]\n",
    "    model_inputs = tokenizer(\n",
    "        input_texts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(l if l != pad_token_id else -100) for l in label]\n",
    "        for label in model_inputs[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d064f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    preprocess_text,\n",
    "    batched=True,\n",
    "    remove_columns=[\"topic\", \"poem\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb92842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6ec0d",
   "metadata": {},
   "source": [
    "##### (2) 파인튜닝 학습 준비\n",
    "\n",
    "- 양자화 설정 > 모델 로드\n",
    "- 학습 모드로 전환\n",
    "- LoRA 학습 설정\n",
    "- TrainingArguments 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf1f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "model.config.attn_implementation = \"flash_attention_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77efb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./q_lora_poem\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d98d7",
   "metadata": {},
   "source": [
    "##### (3) 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519196d6",
   "metadata": {},
   "source": [
    "### 2. 학습된 모델로 시(응답) 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50b255",
   "metadata": {},
   "source": [
    "##### (1) 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qlora_checkpoint = \"./q_lora_poem/checkpoint-xxx\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(qlora_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "generate_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba2c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"바람\", \"비\", \"노을\", \"달빛\", \"안개\", \"사랑\", \"이별\", \"운명\", \"기다림\", \"후회\", \"추억\", \"시간\", \"청춘\", \"변화\", \"마지막 순간\", \"군중\", \"밤거리\", \"버스\", \"인생\", \"빌딩\", \"사람들\", \"거짓말\", \"욕망\", \"돈\", \"권력\", \"비밀\", \"죽음\", \"희망\", \"동물\", \"자연\", \"도시\", \"바다\", \"산\", \"하늘\", \"별\", \"꽃\", \"나무\", \"강\", \"바위\", \"흙\", \"눈\", \"빗방울\", \"눈물\", \"웃음\"]\n",
    "\n",
    "eval_file = \"rlhf_evaluation_data.json\"\n",
    "\n",
    "try:\n",
    "    with open(eval_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        eval_dataset = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    eval_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 5\n",
    "batch_size = 20\n",
    "total_samples = num_batches * batch_size\n",
    "generated_samples = len(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc076994",
   "metadata": {},
   "source": [
    "##### (2) 시 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시 생성 함수 정의\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_poem_batch():\n",
    "    batch_data = []\n",
    "\n",
    "    with tqdm(total=batch_size, desc=\"✍️시 생성 중...\", leave=False) as t:\n",
    "        for _ in range(batch_size):\n",
    "            topic = random.choice(topics)\n",
    "            input_text = f\"주제: {topic}\\n\\n시:\"\n",
    "\n",
    "            start_time = time.time()\n",
    "            poem = generate_pipeline(\n",
    "                input_text,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.8,\n",
    "                top_p=0.9\n",
    "            )[0][\"generated_text\"]\n",
    "            end_time = time.time()\n",
    "\n",
    "            gen_time = end_time - start_time\n",
    "            batch_data.append({\n",
    "                \"topic\": topic,\n",
    "                \"poem\": poem,\n",
    "                \"selected\": None\n",
    "            })\n",
    "\n",
    "            t.update(1)\n",
    "\n",
    "            global generated_samples\n",
    "            generated_samples += 1\n",
    "            complete_rate = (generated_samples / total_samples) * 100\n",
    "            remaining_time = ((total_samples - generated_samples) * gen_time) / 60\n",
    "\n",
    "            print(f\"\\n{generated_samples}/{total_samples}개 완료 ({complete_rate:.2f}%)\")\n",
    "            print(f\" - 예상 남은 시간: {remaining_time:.1f}분\")\n",
    "            print(\"-\" * 100)\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(num_batches), desc=\"<<< 전체 진행 상황 >>>\", position=0):\n",
    "    eval_dataset.extend(generate_poem_batch())\n",
    "\n",
    "    with open(eval_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eval_dataset, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0a5d6",
   "metadata": {},
   "source": [
    "##### (3) 피드백\n",
    "- 생성된 시에 대해 selected=\"True\"로 수정해 피드백 반영"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d25e97",
   "metadata": {},
   "source": [
    "### 3. Reward Model 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76466acd",
   "metadata": {},
   "source": [
    "##### (1) 데이터 로드 및 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03599cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    evaluation_data = json.load(f)\n",
    "\n",
    "reward_data = [\n",
    "    {\"text_a\": f\"주제: {item['topic']},\", \"text_b\": item[\"poem\"]}\n",
    "     for item in evaluation_data if item[\"selected\"]\n",
    "]\n",
    "\n",
    "reward_dataset = Dataset.from_list(reward_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31361be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "def preprocess_reward_data(sample):\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        sample[\"text_a\"],\n",
    "        text_pair=sample[\"text_b\"],\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(l if l != pad_token_id else -100) for l in label]\n",
    "        for label in model_inputs[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4135b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_dataset = reward_dataset.map(\n",
    "    preprocess_reward_data,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text_a\", \"text_b\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80556bf",
   "metadata": {},
   "source": [
    "##### (2) 학습 준비\n",
    "\n",
    "- 양자화 설정 > 모델 로드\n",
    "- LoRA 학습 설정\n",
    "- TrainingArguments 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead62429",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4bc4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b50b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model = prepare_model_for_kbit_training(reward_model)\n",
    "\n",
    "reward_model = get_peft_model(reward_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b8de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_training_args = TrainingArguments(\n",
    "    output_dir=\"./reward_model\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "reward_trainer = Trainer(\n",
    "    model=reward_model,\n",
    "    args=reward_training_args,\n",
    "    train_dataset=reward_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78852cc",
   "metadata": {},
   "source": [
    "##### (3) 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b76e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c45b7",
   "metadata": {},
   "source": [
    "### 4. RLHF (ORPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05503af3",
   "metadata": {},
   "source": [
    "##### (1) 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3469993",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(qlora_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.train()\n",
    "model.cuda()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ddf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5f7f7",
   "metadata": {},
   "source": [
    "##### (2) ORPO 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22785d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    evaluation_data = json.load(f)\n",
    "\n",
    "orpo_data = []\n",
    "\n",
    "for item in evaluation_data:\n",
    "    if item[\"selected\"]:\n",
    "        prompt_text = f\"주제: {item['topic']}\\n\\n이 주제에 맞는 시를 작성해 주세요.\"\n",
    "        chosen_text = item[\"poem\"]\n",
    "        rejected_text = \"\"\n",
    "\n",
    "        tokenized_prompt = tokenizer(prompt_text, truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
    "        tokenized_chosen = tokenizer(chosen_text, truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
    "        tokenized_rejected = tokenizer(rejected_text, truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
    "\n",
    "        orpo_data.append({\n",
    "            \"prompt\": prompt_text,\n",
    "            \"chosen\": chosen_text,\n",
    "            \"rejected\": rejected_text,\n",
    "            \"prompt_input_ids\": tokenized_prompt[\"input_ids\"].squeeze(0).cuda(),\n",
    "            \"prompt_attention_mask\": tokenized_prompt[\"attention_mask\"].squeeze(0).cuda(),\n",
    "            \"chosen_input_ids\": tokenized_chosen[\"input_ids\"].squeeze(0).cuda(),\n",
    "            \"chosen_attention_mask\": tokenized_chosen[\"attention_mask\"].squeeze(0).cuda(),\n",
    "            \"rejected_input_ids\": tokenized_rejected[\"input_ids\"].squeeze(0).cuda(),\n",
    "            \"rejected_attention_mask\": tokenized_rejected[\"attention_mask\"].squeeze(0).cuda()\n",
    "        })\n",
    "\n",
    "        orpo_dataset = Dataset.from_list(orpo_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cafe91",
   "metadata": {},
   "source": [
    "##### (3) ORPO 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import ORPOConfig\n",
    "\n",
    "orpo_config = ORPOConfig(\n",
    "    output_dir='./orpo_output',\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-6,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=50,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e637b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.trainer.utils import DPODataCollatorWithPadding\n",
    "\n",
    "data_collator = DPODataCollatorWithPadding(\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    label_pad_token_id=-100,\n",
    "    is_encoder_decoder=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import ORPOTrainer\n",
    "\n",
    "orpo_trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_config,\n",
    "    train_dataset=orpo_dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a83e8",
   "metadata": {},
   "source": [
    "##### (4) ORPO 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372fd388",
   "metadata": {},
   "outputs": [],
   "source": [
    "orpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26c37b",
   "metadata": {},
   "source": [
    "### 최종 시 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d48229",
   "metadata": {},
   "outputs": [],
   "source": [
    "orpo_checkpoint = \"./orpo_output/checkpoint-xxx\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(orpo_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "generate_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563139d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem_final(num_samples=5):\n",
    "    result = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        topic = random.choice(topics)\n",
    "        input_text = f\"주제: {topic}\\n\\n시:\"\n",
    "\n",
    "        poem = generate_pipeline(\n",
    "            input_text,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        result.append({\"topic\": topic, \"poem\": poem})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_poem = generate_poem_final()\n",
    "generated_poem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
