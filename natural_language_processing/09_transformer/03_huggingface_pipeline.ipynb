{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b7e722",
   "metadata": {},
   "source": [
    "# HuggingFace Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5483b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q kobert-transformers\n",
    "!pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a8fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ccf32",
   "metadata": {},
   "source": [
    "### NLP Tasks\n",
    "\n",
    "주어진 task에서 사전 훈련된 모델을 사용하는 가장 간단한 방법은 `pipeline`을 사용하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "605f6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9af30",
   "metadata": {},
   "source": [
    "##### 기계번역\n",
    "\n",
    "https://huggingface.co/Helsinki-NLP/opus-mt-ko-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccd73e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow<2.20,>=2.19 (from tf-keras)\n",
      "  Downloading tensorflow-2.19.1-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.74.0)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow<2.20,>=2.19->tf-keras)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.11.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 23.2 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.19.1-cp312-cp312-win_amd64.whl (376.0 MB)\n",
      "   ---------------------------------------- 0.0/376.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 17.3/376.0 MB 83.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 35.7/376.0 MB 87.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 43.3/376.0 MB 74.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 52.7/376.0 MB 63.3 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 69.2/376.0 MB 66.9 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 85.7/376.0 MB 69.3 MB/s eta 0:00:05\n",
      "   ---------- ---------------------------- 100.4/376.0 MB 69.7 MB/s eta 0:00:04\n",
      "   ----------- --------------------------- 112.5/376.0 MB 69.0 MB/s eta 0:00:04\n",
      "   ------------- ------------------------- 126.1/376.0 MB 68.2 MB/s eta 0:00:04\n",
      "   ------------- ------------------------- 132.4/376.0 MB 64.5 MB/s eta 0:00:04\n",
      "   ------------- ------------------------- 134.2/376.0 MB 60.8 MB/s eta 0:00:04\n",
      "   ------------- ------------------------- 134.7/376.0 MB 54.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 143.7/376.0 MB 54.0 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 157.0/376.0 MB 54.8 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 172.8/376.0 MB 56.3 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 183.8/376.0 MB 56.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 195.6/376.0 MB 56.3 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 211.6/376.0 MB 57.5 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 224.4/376.0 MB 57.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 235.9/376.0 MB 57.8 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 249.6/376.0 MB 58.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 264.8/376.0 MB 58.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 279.4/376.0 MB 58.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 293.1/376.0 MB 57.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 305.4/376.0 MB 58.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 321.4/376.0 MB 59.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 335.3/376.0 MB 59.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 348.4/376.0 MB 58.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 365.7/376.0 MB 59.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.9/376.0 MB 59.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.9/376.0 MB 59.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 376.0/376.0 MB 53.7 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 83.2 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, tensorboard, tensorflow, tf-keras\n",
      "\n",
      "  Attempting uninstall: protobuf\n",
      "\n",
      "    Found existing installation: protobuf 6.32.0\n",
      "\n",
      "    Uninstalling protobuf-6.32.0:\n",
      "\n",
      "      Successfully uninstalled protobuf-6.32.0\n",
      "\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "  Attempting uninstall: tensorboard\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "    Found existing installation: tensorboard 2.20.0\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "    Uninstalling tensorboard-2.20.0:\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "      Successfully uninstalled tensorboard-2.20.0\n",
      "   ---------------------------------------- 0/4 [protobuf]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "  Attempting uninstall: tensorflow\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "    Found existing installation: tensorflow 2.20.0\n",
      "   ---------- ----------------------------- 1/4 [tensorboard]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "    Uninstalling tensorflow-2.20.0:\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "      Successfully uninstalled tensorflow-2.20.0\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   -------------------- ------------------- 2/4 [tensorflow]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ------------------------------ --------- 3/4 [tf-keras]\n",
      "   ---------------------------------------- 4/4 [tf-keras]\n",
      "\n",
      "Successfully installed protobuf-5.29.5 tensorboard-2.19.0 tensorflow-2.19.1 tf-keras-2.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\~ensorflow'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "296d4d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f43da2708e4ea997a857b2d502d3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-ko-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b428c2e254484231af35d8d4ad426a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5024612a61d14d05a93dc68d5f9bb25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcea406dc4214112b9e093558ebd228e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/842k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb73e173c094f3ba8b3c0bbb8f9f9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/813k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b2b00d6ddc49d3bfa4237ad8e1696d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156f2db723c14248b61338fe8a5d055f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.TranslationPipeline at 0x215a1a7da60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-ko-en')\n",
    "translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f52e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'To the snacks!'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('간식비를 향하여!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820d0eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'I hope you study hard.'},\n",
       " {'translation_text': \"I'd like you to take the class really hard.\"},\n",
       " {'translation_text': \"I'd like you to work hard on your practice.\"},\n",
       " {'translation_text': 'Please listen to 17th century...'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator([\n",
    "    '공부를 열심히 하시면 좋겠어요.',\n",
    "    '수업을 열심히 들으시면 좋겠고요.',\n",
    "    '실습도 열심히 해주시면 좋겠어요.',\n",
    "    '제발 17기 말 좀... 들으세요...'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ea992",
   "metadata": {},
   "source": [
    "##### 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e2cfbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf3ac6912c04f4eb622eec58fe6046f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b77b36e2f84bd4b0aac93499007e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8fd3ca4afd40e6ac6cbed636cb28ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc47546f0d348168c35167e7e902eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_clf = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d7c532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998722076416016}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_clf(\"I'm very happy!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33cbfd",
   "metadata": {},
   "source": [
    "- 한국어\n",
    "\n",
    "https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d084c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6686375a359f4a97a4440127172328bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/932 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--sangrimlee--bert-base-multilingual-cased-nsmc. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1470cc051fe9479f881fb5c8fcaf03e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/712M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e629fea6454449b9a40bd827fc7c3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fe3ab2d000464aaafaea52055dcadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99bb55061e04ec78b9b7b0a72e06eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e56f2b91eaa409991095a672815f9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/711M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24c8dac31944b7a9afe84aaad8335af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_sentiment_clf = pipeline('sentiment-analysis', model='sangrimlee/bert-base-multilingual-cased-nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "706ac9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9047993421554565},\n",
       " {'label': 'negative', 'score': 0.7013086676597595},\n",
       " {'label': 'positive', 'score': 0.7580994367599487}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_sentiment_clf([\n",
    "    '어제 배운 Attention이 어려워서 맥주를 한 잔 했어.',\n",
    "    '공부는 어렵지만 맥주는 시원하더라',\n",
    "    '하지만 나는 굴하지 않고 열심히 공부할거야!!!'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaec0cf",
   "metadata": {},
   "source": [
    "##### Zero Shot Classification\n",
    "\n",
    "- shot == 예시\n",
    "- 예시 없이 (학습 없이) 추론(분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2b98204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8077aa3d3e45a399a846ea1515bec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975b2c15ba8e4963927b2d84331197ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad4eb135b014aed9d6631478e318d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131f485bfb2d42019563cd56b2231442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cade9747338f42d791560d3135c16249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d5c8859593454180478d7697c6f6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "zero_shot_clf = pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f843b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the transformers library.',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9053581953048706, 0.07259626686573029, 0.022045595571398735]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_clf(\n",
    "    \"This is a course about the transformers library.\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f7a260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"It has a poison and it's very dangerous\",\n",
       " 'labels': ['snake', 'tiger', 'squirrel'],\n",
       " 'scores': [0.6373922228813171, 0.19870398938655853, 0.16390381753444672]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_clf(\n",
    "    \"It has a poison and it's very dangerous\",\n",
    "    candidate_labels=[\"tiger\", \"squirrel\", \"snake\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f55e07",
   "metadata": {},
   "source": [
    "- 한국어\n",
    "\n",
    "https://huggingface.co/joeddav/xlm-roberta-large-xnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf790bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de166a96c5a4450b9fb40a7ed0ba9915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/734 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--joeddav--xlm-roberta-large-xnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101d993bbd8c4136a25e2ae2237d379d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae09c5662b2042ee8cb868115bd392ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52da1c180748443593c4b29e43fcebe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7eae2f1a6904dc4b0063dc39ca52a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_zero_shot_clf = pipeline('zero-shot-classification', model='joeddav/xlm-roberta-large-xnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16e03551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '2025년에는 어떤 운동을 하시겠습니까?!',\n",
       " 'labels': ['정치', '건강', '경제'],\n",
       " 'scores': [0.40395545959472656, 0.3669034242630005, 0.22914111614227295]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"2025년에는 어떤 운동을 하시겠습니까?!\"\n",
    "candidate_labels = [\"정치\", \"경제\", \"건강\"]\n",
    "\n",
    "ko_zero_shot_clf(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5676607a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '2025년에는 어떤 운동을 하시겠습니까?!',\n",
       " 'labels': ['건강', '정치', '경제'],\n",
       " 'scores': [0.9117788672447205, 0.05924064293503761, 0.02898053266108036]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"2025년에는 어떤 운동을 하시겠습니까?!\"\n",
    "candidate_labels = [\"정치\", \"경제\", \"건강\"]\n",
    "hypothesis_template = \"이 텍스트는 {}에 관한 내용입니다.\"\n",
    "\n",
    "ko_zero_shot_clf(sequence_to_classify, candidate_labels, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e06a3",
   "metadata": {},
   "source": [
    "##### 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ccf514a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0393b6e6efb4b1ca6e035e8d97f9cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a62ce5bcbb24b8b95b30d32771caaed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5128b09d658e478f93bf6ffb7d770f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4928537f3c249d58cd2e97fae119298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15459ab7e9894fe9beb126577d3e8264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a2d7def0594d9fa9adfd07a429fc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721420bf34da426ab0149e057d9a4aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline('text-generation', model='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f9d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In this course, we will teach you how to use the \\u200cC++ language to build and run applications on C++ platforms, and how to use a language in any language. It will also give you some tips on how to use C++.\\n\\n\\n\\n\\n- There are two types of language: C++ (C++) and C++ (C++) (C++).\\nThe C++ language is a type-neutral language, which means that any language that uses an object type is not bound to that type. The C++ language is not bound to that type, so that all any language that uses a class type is not bound to that type.\\nI will explain a little more in the C++ language.\\nC++ is a language that uses the object type that is not bound to that type. It is a class type that is not bound to that type.\\nThere are two types of C++ that are called C++:\\nA type type that is not bound to that type. A type that is not bound to that type.\\nA type that is not bound to that type. A type that is not bound to that type.\\nThe C++ language is a type that is not bound to that type.\\nA type that is not bound to that type'}, {'generated_text': \"In this course, we will teach you how to generate the code for this course.\\n\\n\\n\\nSo, what are these results?\\nThe result?\\nThe results are the results of the course. We have a number of different values, each of which is different from the other.\\nThe results are shown in the course.\\nNext, we will implement the new function 'Fancy' which can define the numbers of the values.\\nWe are using the function 'Fancy' which can define the numbers of the values.\\nAnd then we can use the function 'Fancy' which will define the numbers of the values.\\nWe are using the function 'Fancy' which can define the numbers of the values.\\nAnd then we are using the function 'Fancy' which can define the numbers of the values.\\nAnd then we are using the function 'Fancy' which is used to create the values for the values.\\nAnd then we are using the function 'Fancy' which is used to create the values for the values.\\nAnd then we are using the function 'Fancy' which is used to create the values for the values.\\nAnd then we are using the function 'Fancy' which is used to create the values for the values.\\nAnd then we\"}]\n"
     ]
    }
   ],
   "source": [
    "text_generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4fe4d",
   "metadata": {},
   "source": [
    "- 한국어\n",
    "\n",
    "https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae860c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fd707b59cc41f7a0bacb3afebd0772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--skt--ko-gpt-trinity-1.2B-v0.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4995c1945da64242959a77df4f7a32de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060383d472004fa0b8360d9393c6c215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270ef9df398c488889454fb8cf41cf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b292c297f9a4569966167f754169421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813350e313634a5bb73decc13dfe803c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_text_generator = pipeline('text-generation', model='skt/ko-gpt-trinity-1.2B-v0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a631f1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '오늘 점심에는 뭐 먹었어\\n 답변:저는 배부르게 먹었답니다. 제가 맛있는 메뉴 추천해드릴께요 메뉴 추천해줘\"라고 말씀해보세요~\"'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ko_text_generator('AI란 말입니다')\n",
    "ko_text_generator('오늘 점심에는')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77abda36",
   "metadata": {},
   "source": [
    "##### Fill Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dcb64f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadb8c3b065545aeb3bd27138fc5397e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e30a670795a4a7f8f1a5e5e29062977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddd313c41cf45c098f62dccc6728e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9f25b487de48289117f9edd399d65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d0135a61944e30afa8cfa5b3ce05db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552a08055dff401380cd693c19cfe961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9248f82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09073042869567871,\n",
       "  'token': 265,\n",
       "  'token_str': ' business',\n",
       "  'sequence': 'This course will teach you all about business model.'},\n",
       " {'score': 0.072625532746315,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical model.'},\n",
       " {'score': 0.049304164946079254,\n",
       "  'token': 5,\n",
       "  'token_str': ' the',\n",
       "  'sequence': 'This course will teach you all about the model.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('This course will teach you all about <mask> model.', top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902fc24",
   "metadata": {},
   "source": [
    "##### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49e2a40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b9f1baf0204cffad5ea7255d66e982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff9261d775b4b44b390d5643b35ad4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d7241e3ad646cbb07f40201fc978c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962322452fd54a1f9cd5753f84ecfbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8df558435f4bffacaeea021182f6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2622def281144c999d30bf500048921b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner', model='dslim/bert-base-NER', grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10a42578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.98995215,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.46767735,\n",
       "  'word': '##qui',\n",
       "  'start': 12,\n",
       "  'end': 15},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.72662497,\n",
       "  'word': '##rrel',\n",
       "  'start': 15,\n",
       "  'end': 19},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.986338,\n",
       "  'word': 'Playdata',\n",
       "  'start': 34,\n",
       "  'end': 42},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9994649,\n",
       "  'word': 'Seoul',\n",
       "  'start': 46,\n",
       "  'end': 51}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner('My name is Squirrel and I work at Playdata in Seoul')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999ca1d",
   "metadata": {},
   "source": [
    "##### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c38e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdf7580927344b3b18d71838191e29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd78c116f9524b778af54a522f824ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a544ff7ac80743f0aae5ad04634100c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ada56e011bd4368a54e77352dee8f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d9fc98c1b74cbebd340e7570def9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "qna = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35bd22e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8710189713747241, 'start': 34, 'end': 42, 'answer': 'Playdata'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Squirrel and I work at Playdata in Seoul\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39969303",
   "metadata": {},
   "source": [
    "- 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "355d35b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f14db0b73844d08264f280ba696caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Playdata\\.cache\\huggingface\\hub\\models--klue--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ef6726ca7f4946b94339a5b4c3dc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9945adab67c545fc9ec0e37f40576a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d267fae15c416b9de8836d7d350a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5696d542a94364a8a73e902ba00a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f755582701b4331885a89adf1ac6025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_qna = pipeline('question-answering', model='klue/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3406dc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0003369680780451745,\n",
       " 'start': 49,\n",
       " 'end': 75,\n",
       " 'answer': '이상 피는 것을 자신의 일과라고 표현했을 정도로'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_qna(\n",
    "    question=\"1931년 어떤 진단을 받았나요?\",\n",
    "    context=\"\"\"그러던 1931년, 그는 갑작스럽게 폐결핵을 진단받았다. 이상은 하루에 담배를 50개피 이상 피는 것을 자신의 일과라고 표현했을 정도로 엄청난 골초였는데, 그 때문인지 병세는 날이 갈수록 악화되어 담당의가 그의 폐를 확인하고는 형체도 안 보인다며 혀를 내둘렀을 정도였다. 결국 1933년부터는 각혈까지 시작되었고, 건축기사 일을 지속하기 어렵다고 판단한 이상은 조선총독부에서 퇴사하고 황해도에 있는 배천 온천으로 요양을 간다.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31c19adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on QuestionAnsweringPipeline in module transformers.pipelines.question_answering object:\n",
      "\n",
      "class QuestionAnsweringPipeline(transformers.pipelines.base.ChunkPipeline)\n",
      " |  QuestionAnsweringPipeline(model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, modelcard: Optional[transformers.modelcard.ModelCard] = None, framework: Optional[str] = None, task: str = '', **kwargs)\n",
      " |\n",
      " |  Question Answering pipeline using any `ModelForQuestionAnswering`. See the [question answering\n",
      " |  examples](../task_summary#question-answering) for more information.\n",
      " |\n",
      " |  Example:\n",
      " |\n",
      " |  ```python\n",
      " |  >>> from transformers import pipeline\n",
      " |\n",
      " |  >>> oracle = pipeline(model=\"deepset/roberta-base-squad2\")\n",
      " |  >>> oracle(question=\"Where do I live?\", context=\"My name is Wolfgang and I live in Berlin\")\n",
      " |  {'score': 0.9191, 'start': 34, 'end': 40, 'answer': 'Berlin'}\n",
      " |  ```\n",
      " |\n",
      " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      " |\n",
      " |  This question answering pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"question-answering\"`.\n",
      " |\n",
      " |  The models that this pipeline can use are models that have been fine-tuned on a question answering task. See the\n",
      " |  up-to-date list of available models on\n",
      " |  [huggingface.co/models](https://huggingface.co/models?filter=question-answering).\n",
      " |\n",
      " |  Arguments:\n",
      " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |      tokenizer ([`PreTrainedTokenizer`]):\n",
      " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          [`PreTrainedTokenizer`].\n",
      " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (`str`, *optional*):\n",
      " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |          installed.\n",
      " |\n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |          provided.\n",
      " |      task (`str`, defaults to `\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      num_workers (`int`, *optional*, defaults to 8):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |          workers to be used.\n",
      " |      batch_size (`int`, *optional*, defaults to 1):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (`int`, *optional*, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |          the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      " |      torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      " |          (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      " |          the raw output data e.g. text.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      QuestionAnsweringPipeline\n",
      " |      transformers.pipelines.base.ChunkPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Answer the question(s) given as inputs by using the context(s).\n",
      " |\n",
      " |      Args:\n",
      " |          question (`str` or `list[str]`):\n",
      " |              One or several question(s) (must be used in conjunction with the `context` argument).\n",
      " |          context (`str` or `list[str]`):\n",
      " |              One or several context(s) associated with the question(s) (must be used in conjunction with the\n",
      " |              `question` argument).\n",
      " |          top_k (`int`, *optional*, defaults to 1):\n",
      " |              The number of answers to return (will be chosen by order of likelihood). Note that we return less than\n",
      " |              top_k answers if there are not enough options available within the context.\n",
      " |          doc_stride (`int`, *optional*, defaults to 128):\n",
      " |              If the context is too long to fit with the question for the model, it will be split in several chunks\n",
      " |              with some overlap. This argument controls the size of that overlap.\n",
      " |          max_answer_len (`int`, *optional*, defaults to 15):\n",
      " |              The maximum length of predicted answers (e.g., only answers with a shorter length are considered).\n",
      " |          max_seq_len (`int`, *optional*, defaults to 384):\n",
      " |              The maximum length of the total sentence (context + question) in tokens of each chunk passed to the\n",
      " |              model. The context will be split in several chunks (using `doc_stride` as overlap) if needed.\n",
      " |          max_question_len (`int`, *optional*, defaults to 64):\n",
      " |              The maximum length of the question after tokenization. It will be truncated if needed.\n",
      " |          handle_impossible_answer (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not we accept impossible as an answer.\n",
      " |          align_to_words (`bool`, *optional*, defaults to `True`):\n",
      " |              Attempts to align the answer to real words. Improves quality on space separated languages. Might hurt on\n",
      " |              non-space-separated languages (like Japanese or Chinese)\n",
      " |\n",
      " |      Return:\n",
      " |          A `dict` or a list of `dict`: Each result comes as a dictionary with the following keys:\n",
      " |\n",
      " |          - **score** (`float`) -- The probability associated to the answer.\n",
      " |          - **start** (`int`) -- The character start index of the answer (in the tokenized version of the input).\n",
      " |          - **end** (`int`) -- The character end index of the answer (in the tokenized version of the input).\n",
      " |          - **answer** (`str`) -- The answer to the question.\n",
      " |\n",
      " |  __init__(self, model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, modelcard: Optional[transformers.modelcard.ModelCard] = None, framework: Optional[str] = None, task: str = '', **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  get_answer(self, answers: list[dict], target: str) -> Optional[dict]\n",
      " |\n",
      " |  get_indices(self, enc: 'tokenizers.Encoding', s: int, e: int, sequence_index: int, align_to_words: bool) -> tuple[int, int]\n",
      " |\n",
      " |  postprocess(self, model_outputs, top_k=1, handle_impossible_answer=False, max_answer_len=15, align_to_words=True)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |\n",
      " |  preprocess(self, example, padding='do_not_pad', doc_stride=None, max_question_len=64, max_seq_len=None)\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |\n",
      " |  span_to_answer(self, text: str, start: int, end: int) -> dict[str, typing.Union[str, int]]\n",
      " |      When decoding from token probabilities, this method maps token indexes to actual word in the initial context.\n",
      " |\n",
      " |      Args:\n",
      " |          text (`str`): The actual context to extract the answer from.\n",
      " |          start (`int`): The answer starting token index.\n",
      " |          end (`int`): The answer end token index.\n",
      " |\n",
      " |      Returns:\n",
      " |          Dictionary like `{'answer': str, 'start': int, 'end': int}`\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  create_sample(question: Union[str, list[str]], context: Union[str, list[str]]) -> Union[transformers.data.processors.squad.SquadExample, list[transformers.data.processors.squad.SquadExample]]\n",
      " |      QuestionAnsweringPipeline leverages the [`SquadExample`] internally. This helper method encapsulate all the\n",
      " |      logic for converting question(s) and context(s) to [`SquadExample`].\n",
      " |\n",
      " |      We currently support extractive question answering.\n",
      " |\n",
      " |      Arguments:\n",
      " |          question (`str` or `list[str]`): The question(s) asked.\n",
      " |          context (`str` or `list[str]`): The context(s) in which we will look for the answer.\n",
      " |\n",
      " |      Returns:\n",
      " |          One or a list of [`SquadExample`]: The corresponding [`SquadExample`] grouping question and context.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  default_input_names = 'question,context'\n",
      " |\n",
      " |  handle_impossible_answer = False\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.ChunkPipeline:\n",
      " |\n",
      " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |\n",
      " |  check_model_type(self, supported_models: Union[list[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |\n",
      " |      Args:\n",
      " |          supported_models (`list[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |\n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |\n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |\n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |\n",
      " |      Return:\n",
      " |          `dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |\n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |\n",
      " |  get_inference_context(self)\n",
      " |\n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |\n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str from transformers.utils.hub.PushToHubMixin\n",
      " |      Upload the pipeline file to the 🤗 Model Hub.\n",
      " |\n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your pipe to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload pipe\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `hf auth login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |          tags (`list[str]`, *optional*):\n",
      " |              List of tags to push on the Hub.\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      from transformers import pipeline\n",
      " |\n",
      " |      pipe = pipeline(\"google-bert/bert-base-cased\")\n",
      " |\n",
      " |      # Push the pipe to your namespace with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"my-finetuned-bert\")\n",
      " |\n",
      " |      # Push the pipe to an organization with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |\n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = True, **kwargs)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |\n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |          kwargs (`dict[str, Any]`, *optional*):\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |\n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.pipelines.base.Pipeline:\n",
      " |\n",
      " |  torch_dtype\n",
      " |      Torch dtype of the model (if it's Pytorch model), `None` otherwise.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ko_qna)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
