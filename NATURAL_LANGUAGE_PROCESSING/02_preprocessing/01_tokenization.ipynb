{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df573da",
   "metadata": {},
   "source": [
    "# 토큰화 (Tokenization)\n",
    "\n",
    "- 문장이나 단어를 더 작은 단위로 나누어 분석 가능한 단위(토큰, Token)으로 변환하는 과정\n",
    "- 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 혹은 처리하는 단위로써 토큰 정의\n",
    "- 자연어 처리에서 크롤링, 데이터 수집 등으로 얻은 코퍼스 데이터는 정제되지 않은 경우가 많은데 이를 사용 용도에 맞게 토큰화, 정제, 정규화하는 과정이 필요\n",
    "\n",
    "**토큰화 목적**\n",
    "\n",
    "- 문법적 구조 이해\n",
    "- 유연한 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dbaee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 기본적인 토큰 처리 지원\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77a52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP is fascinating. It has many applications in real-world scenarios.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "697d4426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinating', '.', 'It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']\n",
      "['NLP is fascinating.', 'It has many applications in real-world scenarios.']\n",
      "['NLP', 'is', 'fascinating', '.']\n",
      "['It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 단어 토큰화\n",
    "print(nltk.word_tokenize(text))\n",
    "\n",
    "# 문장 토큰화\n",
    "print(nltk.sent_tokenize(text))\n",
    "\n",
    "# 문장별 단어 토큰화 \n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    print(nltk.word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80d79c",
   "metadata": {},
   "source": [
    "### Subword Tokenization\n",
    "\n",
    "- BertTokenizer\n",
    "    - 단어를 부분 단위로 쪼개어 희귀하거나 새로운 단어도 부분적으로 표현할 수 있도록 함 -> 어휘 크기를 줄이고 다양한 언어 패턴 학습 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "017abfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7ad603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['un', '##ha', '##pp', '##iness']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # 사전 학습된 BertTokenizer 로드\n",
    "# word = 'happy'\n",
    "word = 'unhappiness'\n",
    "subwords = tokenizer.tokenize(word)\n",
    "subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a59820f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nl',\n",
       " '##p',\n",
       " 'is',\n",
       " 'fascinating',\n",
       " '.',\n",
       " 'it',\n",
       " 'has',\n",
       " 'many',\n",
       " 'applications',\n",
       " 'in',\n",
       " 'real',\n",
       " '-',\n",
       " 'world',\n",
       " 'scenarios',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0d893",
   "metadata": {},
   "source": [
    "### 문자 단위 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5794acd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b252dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'an',\n",
       " 'arrow',\n",
       " 'fruit',\n",
       " 'flies',\n",
       " 'like',\n",
       " 'a',\n",
       " 'banana']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Time flies like an arrow; fruit flies like a banana.\"\n",
    "re.findall(r'\\b\\w+\\b', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15dd5de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'use', 'well', '-', 'being', 'practices', 'for', 'self', '-', 'care', '.']\n",
      "['Do', \"n't\", 'hesitate', 'to', 'use', 'well-being', 'practices', 'for', 'self-care', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer: 단어/구두점으로 토큰을 구분 (', - 포함 단어도 분리)\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "\n",
    "text = \"Don't hesitate to use well-being practices for self-care.\"\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print(word_punct_tokenizer.tokenize(text))\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4520cbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/08/18', '날짜', '표현에', '사용될', '수', '있다.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n",
      "['COVID-19', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다', '.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/08/18', '날짜', '표현에', '사용될', '수', '있다', '.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n"
     ]
    }
   ],
   "source": [
    "# TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, word_tokenize\n",
    "\n",
    "text = '''\n",
    "COVID-19(전염병), Dr.Smith(의사), NASA(우주항공국) 등 특정 기관이나 명칭이 있다.\n",
    "특수 문자 또한 태그 <br>, 가격 $100.50, 2025/08/18 날짜 표현에 사용될 수 있다.\n",
    "이러한 경우, $100.50을 하나의 토큰으로 유지할 필요가 있다.\n",
    "'''\n",
    "\n",
    "treebank_word_tokenizer = TreebankWordTokenizer()\n",
    "print(treebank_word_tokenizer.tokenize(text))\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7ba3b",
   "metadata": {},
   "source": [
    "### 한국어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6a9fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kss==5.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adeeb3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n",
      "c:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\nlp_env\\Lib\\site-packages\\pecab\\_tokenizer.py:265: RuntimeWarning: overflow encountered in scalar add\n",
      "  from_pos_data.costs[idx]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['시간적 배경은 1920년대의 겨울로, 공간적 배경은 경성부.',\n",
       " '주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다.',\n",
       " \"아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다'는 김 첨지의 신조 때문으로 나오지만 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다\",\n",
       " '는 이유가 더 크다.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kss (Korean Sentence Splitter)\n",
    "import kss\n",
    "\n",
    "text = \"시간적 배경은 1920년대의 겨울로, 공간적 배경은 경성부. 주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다. 아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다'는 김 첨지의 신조 때문으로 나오지만 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.\"\n",
    "\n",
    "kss.split_sentences(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69fd2ff",
   "metadata": {},
   "source": [
    "### 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd7f2f",
   "metadata": {},
   "source": [
    "**pos_tag**\n",
    "\n",
    "pos_tag는 자연어 처리(NLP)에서 단어에 품사를 태깅하는 함수로, 주로 NLTK와 같은 라이브러리에서 사용된다.\n",
    "\n",
    "- nltk pos_tag() 주요 품사 태깅<br>\n",
    "\n",
    "1. **NN (Noun, Singular)**  \n",
    "   단수 명사를 나타낸다. 하나의 사물이나 개념을 지칭한다.  \n",
    "   예시: \"cat\", \"book\", \"apple\"\n",
    "\n",
    "2. **NNS (Noun, Plural)**  \n",
    "   복수 명사를 나타낸다. 두 개 이상의 사물이나 개념을 지칭한다.  \n",
    "   예시: \"cats\", \"books\", \"apples\"\n",
    "\n",
    "3. **NNP (Proper Noun, Singular)**  \n",
    "   단수 고유 명사를 나타낸다. 특정한 사람, 장소 또는 조직의 이름을 지칭한다.  \n",
    "   예시: \"Alice\", \"London\", \"NASA\"\n",
    "\n",
    "4. **NNPS (Proper Noun, Plural)**  \n",
    "   복수 고유 명사를 나타낸다. 두 개 이상의 특정한 사람, 장소 또는 조직의 이름을 지칭한다.  \n",
    "   예시: \"Smiths\", \"United Nations\"\n",
    "\n",
    "5. **VB (Verb, Base Form)**  \n",
    "   동사의 원형을 나타낸다. 일반적으로 현재 시제와 함께 사용된다.  \n",
    "   예시: \"run\", \"eat\", \"play\"\n",
    "\n",
    "6. **VBD (Verb, Past Tense)**  \n",
    "   동사의 과거형을 나타낸다.  \n",
    "   예시: \"ran\", \"ate\", \"played\"\n",
    "\n",
    "7. **VBG (Verb, Gerund or Present Participle)**  \n",
    "   동명사 또는 현재 분사를 나타낸다. 일반적으로 \"-ing\" 형태이다.  \n",
    "   예시: \"running\", \"eating\", \"playing\"\n",
    "\n",
    "8. **VBN (Verb, Past Participle)**  \n",
    "   동사의 과거 분사형을 나타낸다. 주로 완료 시제와 함께 사용된다.  \n",
    "   예시: \"run\" (as in \"has run\"), \"eaten\", \"played\"\n",
    "\n",
    "9. **VBZ (Verb, 3rd Person Singular Present)**  \n",
    "   3인칭 단수 현재형 동사를 나타낸다. 주어가 3인칭 단수일 때 사용된다.  \n",
    "   예시: \"runs\", \"eats\", \"plays\"\n",
    "\n",
    "10. **JJ (Adjective)**  \n",
    "    형용사를 나타낸다. 명사를 수식하여 그 특성을 설명한다.  \n",
    "    예시: \"big\", \"blue\", \"happy\"\n",
    "\n",
    "11. **JJR (Adjective, Comparative)**  \n",
    "    비교급 형용사를 나타낸다. 두 개의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"bigger\", \"bluer\", \"happier\"\n",
    "\n",
    "12. **JJS (Adjective, Superlative)**  \n",
    "    최상급 형용사를 나타낸다. 세 개 이상의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"biggest\", \"bluest\", \"happiest\"\n",
    "\n",
    "13. **RB (Adverb)**  \n",
    "    부사를 나타낸다. 동사, 형용사 또는 다른 부사를 수식한다.  \n",
    "    예시: \"quickly\", \"very\", \"well\"\n",
    "\n",
    "14. **RBR (Adverb, Comparative)**  \n",
    "    비교급 부사를 나타낸다. 두 개의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"more quickly\", \"better\"\n",
    "\n",
    "15. **RBS (Adverb, Superlative)**  \n",
    "    최상급 부사를 나타낸다. 세 개 이상의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"most quickly\", \"best\"\n",
    "\n",
    "16. **IN (Preposition or Subordinating Conjunction)**  \n",
    "    전치사 또는 종속 접속사를 나타낸다. 명사와의 관계를 나타내거나 종속절을 시작한다.  \n",
    "    예시: \"in\", \"on\", \"because\"\n",
    "\n",
    "17. **DT (Determiner)**  \n",
    "    한정사를 나타낸다. 명사의 수와 상태를 정의한다.  \n",
    "    예시: \"the\", \"a\", \"some\"\n",
    "\n",
    "18. **PRP (Personal Pronoun)**  \n",
    "    인칭 대명사를 나타낸다. 사람, 사물 등을 대체할 때 사용된다.  \n",
    "    예시: \"I\", \"you\", \"he\", \"they\"\n",
    "\n",
    "19. **PRP$ (Possessive Pronoun)**  \n",
    "    소유 대명사를 나타낸다. 소유 관계를 나타낸다.  \n",
    "    예시: \"my\", \"your\", \"his\", \"their\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d058e98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fefb7750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('arrow', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"Time flies like an arrow.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78b1bb",
   "metadata": {},
   "source": [
    "**spacy 주요 품사 태깅**\n",
    "\n",
    "| 태그 | 설명                 | 예시                 |\n",
    "|------|----------------------|----------------------|\n",
    "| ADJ  | 형용사               | big, nice           |\n",
    "| ADP  | 전치사               | in, to, on          |\n",
    "| ADV  | 부사                 | very, well          |\n",
    "| AUX  | 조동사               | is, have (조동사로 사용될 때) |\n",
    "| CONJ | 접속사               | and, or             |\n",
    "| DET  | 한정사/관사          | the, a              |\n",
    "| INTJ | 감탄사               | oh, wow             |\n",
    "| NOUN | 명사                 | dog, table          |\n",
    "| NUM  | 숫자                 | one, two, 3         |\n",
    "| PART | 소사                 | 'to' (to fly에서), not |\n",
    "| PRON | 대명사               | he, she, it         |\n",
    "| PROPN| 고유명사             | John, France        |\n",
    "| PUNCT| 구두점               | ., !, ?             |\n",
    "| SCONJ| 종속 접속사          | because, if         |\n",
    "| SYM  | 기호                 | $, %, @             |\n",
    "| VERB | 동사                 | run, eat            |\n",
    "| X    | 알 수 없는 품사       | 외국어 단어, 잘못된 형식 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4999fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9442fc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.cli.download('en_core_web_sm')        # 영어모델 다운로드 (사용 전 1회는 반드시 다운로드)\n",
    "spacy_nlp = spacy.load('en_core_web_sm')    # 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "599bea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : NOUN\n",
      "flies : VERB\n",
      "like : ADP\n",
      "an : DET\n",
      "arrow : NOUN\n",
      ". : PUNCT\n"
     ]
    }
   ],
   "source": [
    "tokens = spacy_nlp(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, \":\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a71191",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "\n",
    "- 한국어 자연어 처리를 위한 라이브러리\n",
    "- 형태소 분석, 품사 태깅, 텍스트 전처리 등 기능 지원\n",
    "- 여러 형태소 분석기 중 적합한 분석기 선택 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9c75101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022051b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '점심', '은', '뭘', '먹어', '볼까', '.', '맛있는', '게', '뭐', '지', '?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "text = \"오늘 점심은 뭘 먹어볼까. 맛있는 게 뭐지?\"\n",
    "\n",
    "okt = Okt()     # konlpy가 제공하는 한국어 형태소 분석기를 사용하기 위해서는 JVM이 필요함 (JDK 설치 후 정상 작동)\n",
    "\n",
    "# 형태소 분석\n",
    "morphs = okt.morphs(text)\n",
    "morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be47398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'Noun'),\n",
       " ('점심', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('뭘', 'Noun'),\n",
       " ('먹어', 'Verb'),\n",
       " ('볼까', 'Verb'),\n",
       " ('.', 'Punctuation'),\n",
       " ('맛있는', 'Adjective'),\n",
       " ('게', 'Noun'),\n",
       " ('뭐', 'Noun'),\n",
       " ('지', 'Josa'),\n",
       " ('?', 'Punctuation')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 품사 태깅\n",
    "pos_tags = okt.pos(text)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7e05553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '점심', '뭘', '게', '뭐']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명사 추출\n",
    "nouns = okt.nouns(text)\n",
    "nouns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
